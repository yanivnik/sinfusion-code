{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from common_utils.notebook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from common_utils.resize_right import resize\n",
    "from datasets.cropset import CropSet\n",
    "from diffusion.diffusion import Diffusion\n",
    "from diffusion.conditional_diffusion import ConditionalDiffusion\n",
    "from diffusion.sr_diffusion import SRDiffusion\n",
    "from diffusion.diffusion_utils import save_diffusion_sample\n",
    "from models.zssr import ZSSRNet\n",
    "from models.unet import Unet\n",
    "from common_utils.video import html_vid\n",
    "from models.nextnet import NextNet\n",
    "from metrics.sifid_score import get_sifid_scores\n",
    "from common_utils.ben_image import imread\n",
    "from models.modules import *\n",
    "import torchvision.io\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample, figsize_mult=5):\n",
    "    s = (sample.clamp(-1, 1) + 1) / 2\n",
    "    s = (s * 255).type(torch.uint8).moveaxis(1, 3)\n",
    "    s = s.cpu().numpy()\n",
    "    \n",
    "    grid_h = int(len(s) ** 0.5)\n",
    "    grid_w = len(s) // grid_h\n",
    "    \n",
    "    if len(s) > 1:\n",
    "        f, axarr = plt.subplots(grid_h, grid_w, figsize=(figsize_mult * grid_h, figsize_mult * grid_w))\n",
    "        \n",
    "        for idx, img in enumerate(s):\n",
    "            if grid_h == 1:\n",
    "                axarr[idx].imshow(img)\n",
    "            else:\n",
    "                axarr[idx // grid_h, idx % grid_h].imshow(img)\n",
    "    else:\n",
    "        plt.figure(figsize=None if figsize_mult==5 else (figsize_mult, figsize_mult))\n",
    "        plt.imshow(s[0])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def make_gif(samples, output_path):\n",
    "    s = (samples.clamp(-1, 1) + 1) / 2\n",
    "    s = (s * 255).type(torch.uint8).moveaxis(1, -1)\n",
    "    imageio.mimsave(output_path, list(s.cpu().numpy()), fps=5)\n",
    "    \n",
    "def show_gif(samples, interval=25):\n",
    "    vid = samples.transpose(1,0).unsqueeze(0)\n",
    "    anim = html_vid(vid, interval=interval)\n",
    "    display(HTML(f\"\"\"<table><tr><td>{anim.to_html5_video()}</td></tr></table>\"\"\"))\n",
    "    \n",
    "def combine_images(main_img, addition_img, location):\n",
    "    assert len(main_img.shape) == 3\n",
    "    assert addition_img.shape[0] == 4, 'Additional image must have an opacity channel'\n",
    "    combined_img = main_img.clone()\n",
    "    opacity = addition_img[3]\n",
    "    combined_img[:, location[0]:location[0] + addition_img.shape[-2], location[1]:location[1] + addition_img.shape[-1]] = \\\n",
    "        (combined_img[:, location[0]:location[0] + addition_img.shape[-2], location[1]:location[1] + addition_img.shape[-1]]) * (1 - opacity) + addition_img[:3, :, :] * opacity\n",
    "    return combined_img\n",
    "\n",
    "def save_sample(sample, output_path):\n",
    "    s = (sample.clamp(-1, 1) + 1) / 2\n",
    "    s = (s * 255).type(torch.uint8).moveaxis(1, 3).cpu().numpy()\n",
    "    Image.fromarray(s[0]).save(output_path)\n",
    "    \n",
    "def noise_img(img, model, t):\n",
    "    batch_size = img.shape[0]\n",
    "    if isinstance(model, Diffusion):\n",
    "        noisy_img = model.q_sample(img, t)\n",
    "    elif isinstance(model, ConditionalDiffusion):\n",
    "        continuous_sqrt_alpha_hat = torch.FloatTensor(np.random.uniform(model.sqrt_alphas_hat_prev[t - 1], model.sqrt_alphas_hat_prev[t], size=batch_size)).to(img.device).view(batch_size, -1)\n",
    "        noisy_img = model.q_sample(img, continuous_sqrt_alpha_hat.view(-1, 1, 1, 1))\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    return noisy_img\n",
    "    \n",
    "def torchvid2mp4(vid, path, fps=10):\n",
    "    \"\"\" vid is CTHW \"\"\"\n",
    "    torchvision.io.write_video(path, tensor2npimg(vid[None, ...], to_numpy=False).permute(1, 2, 3, 0), fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Method - Pyramid Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Debugging - Attempt to improve level0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/57_nextnet_all_layers-500-ts-much-faster-sampling/checkpoints'\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), \n",
    "                                          model=NextNet(), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "s0 = level0.sample((24, 33), batch_size=16)\n",
    "#s0 = level0.sample((100, 100), batch_size=1)\n",
    "show_sample(s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Full sampling attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/57_nextnet_all_layers-500-ts-much-faster-sampling/checkpoints'\n",
    "image_name = 'balloons.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), \n",
    "                                                model=NextNet(), timesteps=500).to(device='cuda:0')\n",
    "level1 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=1-step=34999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500, strict=False).to(device='cuda:0')\n",
    "level2 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=2-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500, strict=False).to(device='cuda:0')\n",
    "level3 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=3-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500, strict=False).to(device='cuda:0')\n",
    "level4 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=4-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500, strict=False).to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = 1\n",
    "sizes = [(186, 248)]\n",
    "for i in range(1, levels):\n",
    "    sizes.insert(0, (int(sizes[0][0] * size_ratios), int(sizes[0][1] * size_ratios)))\n",
    "\n",
    "s0 = level0.sample(image_size=sizes[0], batch_size=batch)\n",
    "show_sample(s0)\n",
    "s0_r = resize(s0, out_shape=sizes[1])\n",
    "\n",
    "s1 = level1.sample(s0_r)\n",
    "show_sample(s1)\n",
    "s1_r = resize(s1, out_shape=sizes[2])\n",
    "\n",
    "s2 = level2.sample(s1_r)\n",
    "show_sample(s2)\n",
    "s2_r = resize(s2, out_shape=sizes[3])\n",
    "\n",
    "s3 = level3.sample(s2_r)\n",
    "show_sample(s3)\n",
    "s3_r = resize(s3, out_shape=sizes[4])\n",
    "\n",
    "s4 = level4.sample(s3_r)\n",
    "show_sample(s4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from metrics.sifid_score import get_sifid_scores\n",
    "from common_utils.ben_image import imread\n",
    "\n",
    "orig_image = imread(f'images/{image_name}').to(device=device)\n",
    "normalized_samples = (s4.clamp(-1, 1).unsqueeze(1) + 1) / 2\n",
    "scores =  get_sifid_scores(orig_image, normalized_samples)\n",
    "print('SIFID ', scores.mean(), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Debugging - DDIM sampling attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = 1\n",
    "levels = 5\n",
    "image_name = 'balloons.png'\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "sizes = [(186, 248)]\n",
    "for i in range(1, levels):\n",
    "    sizes.insert(0, (int(sizes[0][0] * size_ratios), int(sizes[0][1] * size_ratios)))\n",
    "    \n",
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/60_nextnet_all_layers-500-ts-with-1.0-ddim-recon-loss-recontrain/checkpoints'\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), model=NextNet(), timesteps=500).to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_T = torch.randn((batch, 3, sizes[0][0], sizes[0][1]), device='cuda:0')\n",
    "    #s0 = level0.sample_ddim(x_T=x_T, sampling_step_size=100)\n",
    "    #show_sample(s0)\n",
    "    #s0 = level0.sample_ddim(x_T=x_T, sampling_step_size=50)\n",
    "    #show_sample(s0)\n",
    "    s0 = level0.sample_ddim(x_T=x_T, sampling_step_size=10)\n",
    "    show_sample(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_T2 = torch.randn((batch, 3, sizes[0][0], sizes[0][1]), device='cuda:0')\n",
    "    #s0 = level0.sample_ddim(x_T=x_T2, sampling_step_size=100)\n",
    "    #show_sample(s0)\n",
    "    #s0 = level0.sample_ddim(x_T=x_T2, sampling_step_size=50)\n",
    "    #show_sample(s0)\n",
    "    s0 = level0.sample_ddim(x_T=x_T2, sampling_step_size=10)\n",
    "    show_sample(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Manual linear interpolation\n",
    "\n",
    "interp_size = 100\n",
    "samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(interp_size):\n",
    "        x_T_interp = x_T * (1 - (i / (interp_size - 1))) + x_T2 * (i / (interp_size - 1)) \n",
    "        s0 = level0.sample_ddim(x_T=x_T_interp, sampling_step_size=10)\n",
    "        samples.append(s0.unsqueeze(0).cpu())\n",
    "\n",
    "samples = torch.cat(samples, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_gif(samples, '/home/yanivni/data/tmp/ddim_interp_trained_on_recon.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run the above 100 times to generate 100 gifs and understand if what we see is accidental or shows a cool phenomena\n",
    "gif_count = 100\n",
    "batch = 1\n",
    "out_dir = '/home/yanivni/data/tmp/ddim_interp_trained_on_recon'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for gif_index in range(gif_count):\n",
    "        x_T = torch.randn((batch, 3, sizes[0][0], sizes[0][1]), device='cuda:0')\n",
    "        x_T2 = torch.randn((batch, 3, sizes[0][0], sizes[0][1]), device='cuda:0')\n",
    "        s0 = level0.sample_ddim(x_T=x_T, sampling_step_size=10)\n",
    "        s0_2 = level0.sample_ddim(x_T=x_T2, sampling_step_size=10)\n",
    "        \n",
    "        save_sample(s0, output_path=fr'{out_dir}/{gif_index}_a1.png')\n",
    "        save_sample(s0_2, output_path=fr'{out_dir}/{gif_index}_a2.png')\n",
    "        \n",
    "        # Interpolate\n",
    "        interp_size = 100\n",
    "        samples = []\n",
    "        for i in range(interp_size):\n",
    "            x_T_interp = x_T * (1 - (i / (interp_size - 1))) + x_T2 * (i / (interp_size - 1))\n",
    "            s_interp = level0.sample_ddim(x_T=x_T_interp, sampling_step_size=10)\n",
    "            samples.append(s_interp.unsqueeze(0).cpu())\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "\n",
    "        make_gif(samples, f'{out_dir}/{gif_index}_gif.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DDIM spherical linear interpolation\n",
    "samples = level0.sample_interpolate(image_size=sizes[0], batch_size=2, interp_seq_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(samples.shape[0]):\n",
    "    make_gif(samples[i], f'/home/yanivni/data/tmp/ddim_interp_trained_on_recon_spherical/{i}.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Experiment - Deterministic noising and denoising process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/58_nextnet_all_layers-500-ts-with-0.8-ddim-recon-loss-0.2-recontrain/checkpoints'\n",
    "image_name = 'balloons.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), model=NextNet(), timesteps=500)#.to(device='cuda:0')\n",
    "\n",
    "img_level0 = imread(f'./images/balloons_level0.png')#.to(device=device)\n",
    "img_level0_normalized = (img_level0 * 2) - 1\n",
    "img_level0_rotated = img_level0_normalized.roll(40, dims=[3])\n",
    "show_sample(img_level0_normalized)\n",
    "show_sample(img_level0_rotated)\n",
    "\n",
    "t = torch.full((1, ), 249, dtype=torch.int64)#, device=device)\n",
    "noise = torch.randn_like(img_level0)\n",
    "x_noisy_1 = level0.q_sample(x_start=img_level0_normalized, t=t, noise=noise)\n",
    "x_noisy_2 = level0.q_sample(x_start=img_level0_rotated, t=t, noise=noise)\n",
    "\n",
    "with torch.no_grad():\n",
    "    s0_1 = level0.sample_ddim(x_T=x_noisy_1, sampling_step_size=10)\n",
    "    s0_2 = level0.sample_ddim(x_T=x_noisy_2, sampling_step_size=10)\n",
    "    show_sample(x_noisy_1)\n",
    "    show_sample(s0_1)\n",
    "    show_sample(s0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples = level0.sample_interpolate(image_size=sizes[0], batch_size=1, interp_seq_len=100, x_T1=x_noisy_1, x_T2=x_noisy_2)\n",
    "make_gif(samples[0], f'/home/yanivni/data/tmp/ddim_interp_trained_on_recon_spherical/move_balloons.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Experiment - try denoising a noised version of image A from model trained on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model trained on balloons, sampled on lightning\n",
    "\n",
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/60_nextnet_all_layers-500-ts-with-1.0-ddim-recon-loss-recontrain/checkpoints'\n",
    "image_name = 'balloons.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "sizes = [(186, 248)]\n",
    "for i in range(1, levels):\n",
    "    sizes.insert(0, (int(sizes[0][0] * size_ratios), int(sizes[0][1] * size_ratios)))\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), model=NextNet(), timesteps=500)#.to(device='cuda:0')\n",
    "\n",
    "img_level0 = imread(f'./images/lightning1_level0.png')\n",
    "img_level0_normalized = (img_level0 * 2) - 1\n",
    "show_sample(img_level0_normalized)\n",
    "t = 150\n",
    "t_tensor = torch.full((1, ), t, dtype=torch.int64)\n",
    "x_noisy_1 = level0.q_sample(x_start=img_level0_normalized, t=t_tensor)\n",
    "with torch.no_grad():\n",
    "    s0_1 = level0.sample(image_size=sizes[0], batch_size=1, custom_initial_img=x_noisy_1, custom_timesteps=t)\n",
    "    show_sample(x_noisy_1)\n",
    "    show_sample(s0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Model trained on starry night, sampled on balloons (this cell is just to show some starry night samples, next cell is the style transfer attempt)\n",
    "\n",
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/starry_night.png/version_1/checkpoints'\n",
    "image_name = 'starry_night.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "sizes = [(201, 256)]\n",
    "for i in range(1, levels):\n",
    "    sizes.insert(0, (int(sizes[0][0] * size_ratios), int(sizes[0][1] * size_ratios)))\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), model=NextNet(), timesteps=500)\n",
    "s0 = level0.sample(image_size=sizes[0], batch_size=4)\n",
    "show_sample(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_level0 = imread(f'./images/balloons_level0.png')\n",
    "img_level0_normalized = (img_level0 * 2) - 1\n",
    "show_sample(img_level0_normalized)\n",
    "t = 40\n",
    "t_tensor = torch.full((1, ), t, dtype=torch.int64)\n",
    "x_noisy_1 = level0.q_sample(x_start=img_level0_normalized, t=t_tensor)\n",
    "with torch.no_grad():\n",
    "    s0_1 = level0.sample(image_size=sizes[0], batch_size=1, custom_initial_img=x_noisy_1, custom_timesteps=t)\n",
    "    show_sample(x_noisy_1)\n",
    "    show_sample(s0_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Experiment - try denoising with a linear combination of weights from two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "starry_night = Diffusion.load_from_checkpoint(r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/starry_night.png/version_1/checkpoints/level=0-step=29999.ckpt', model=NextNet(), timesteps=500)\n",
    "balloons = Diffusion.load_from_checkpoint(r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/58_nextnet_all_layers-500-ts-with-0.8-ddim-recon-loss-0.2-recontrain/checkpoints/level=0-step=29999.ckpt', model=NextNet(), timesteps=500)\n",
    "\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "sizes = [(200, 256)]\n",
    "for i in range(1, levels):\n",
    "    sizes.insert(0, (int(sizes[0][0] * size_ratios), int(sizes[0][1] * size_ratios)))\n",
    "    \n",
    "def interpolate_model(model_a, model_b):\n",
    "    interpolated_model = copy.deepcopy(model_a)\n",
    "    interpolated_state_dict = interpolated_model.state_dict()\n",
    "\n",
    "    for k, v in interpolated_model.named_parameters():\n",
    "        interpolated_state_dict[k] = (model_a.state_dict()[k] + model_b.state_dict()[k]) / 2\n",
    "    \n",
    "    interpolated_model.load_state_dict(interpolated_state_dict)\n",
    "    return interpolated_model\n",
    "\n",
    "def half_layers(model_a, model_b):\n",
    "    new_model = copy.deepcopy(model_a)\n",
    "    new_state_dict = new_model.state_dict()\n",
    "\n",
    "    for k, v in new_model.named_parameters():\n",
    "        if any([k.startswith(f'model.layers.{i}') for i in range(0, 1)]):\n",
    "            new_state_dict[k] = model_b.state_dict()[k]\n",
    "    \n",
    "    new_model.load_state_dict(new_state_dict)\n",
    "    return new_model\n",
    "    \n",
    "interpolated_model = interpolate_model(balloons, starry_night)\n",
    "interpolated_model2 = interpolate_model(starry_night, balloons)\n",
    "half_model1 = half_layers(balloons, starry_night)\n",
    "half_model2 = half_layers(starry_night, balloons)\n",
    "    \n",
    "s0 = half_model1.sample(image_size=sizes[0], batch_size=4)\n",
    "show_sample(s0)\n",
    "s0 = half_model2.sample(image_size=sizes[0], batch_size=4)\n",
    "show_sample(s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Attempt - improving SR layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/59_nextnet_all_layers-500-ts-with-1.0-ddim-recon-loss-full-recontrain/checkpoints'\n",
    "image_name = 'balloons.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "batch = 1\n",
    "sizes = [(186, 248)]\n",
    "imgs = [(imread(f'./images/{image_name}').to(device=device) * 2) - 1]\n",
    "\n",
    "for i in range(1, levels):\n",
    "    imgs.insert(0, resize(imgs[0], scale_factors=size_ratios))\n",
    "    sizes.insert(0, imgs[0].shape[-2:])\n",
    "    \n",
    "    \n",
    "show_factor = 10 # Just some constant for the show_sample func\n",
    "\n",
    "print(imgs[1].mean())\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), \n",
    "                                                model=NextNet(), timesteps=500).to(device='cuda:0')\n",
    "level1 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=1-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=9), recon_loss_factor=1, recon_image=imgs[1], recon_image_lr=resize(imgs[0], out_shape=imgs[1].shape), timesteps=500).to(device='cuda:0')\n",
    "print(imgs[1].mean())\n",
    "print(((imgs[1] * 2) - 1).mean())\n",
    "level2 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=2-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=9), recon_loss_factor=1, recon_image=imgs[2], recon_image_lr=resize(imgs[1], out_shape=imgs[2].shape), timesteps=500).to(device='cuda:0')\n",
    "level3 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=3-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=9), timesteps=500, strict=False).to(device='cuda:0')\n",
    "level4 = TheirsSRDiffusion.load_from_checkpoint(os.path.join('/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/57_nextnet_all_layers-500-ts-much-faster-sampling/checkpoints', 'level=4-step=29999.ckpt'), \n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    #s0 = level0.sample(image_size=sizes[0], batch_size=batch)\n",
    "    s0 = imgs[0]\n",
    "    s0_r = resize(s0, out_shape=sizes[1])\n",
    "    show_sample(torch.cat([s0, imgs[0]]), show_factor)\n",
    "    print(sifid(0, s0))\n",
    "\n",
    "    s1_ddpm = level1.sample(resize(s0, out_shape=sizes[1]), image_size=sizes[1])\n",
    "    #s1 = level1.sample_ddim(lr=s0_r, x_T=torch.randn_like(s0_r), sampling_step_size=5)\n",
    "    s1 = level1.sample_ddim(lr=s0_r, x_T=level1.recon_noise, sampling_step_size=10)\n",
    "    s1_r = resize(s1, out_shape=sizes[2])\n",
    "    show_sample(torch.cat([s1, s1_ddpm, imgs[1]]), show_factor)\n",
    "    print(sifid(1, s1), sifid(1, s1_ddpm))\n",
    "\n",
    "    s2_ddpm = level2.sample(resize(s1_ddpm, out_shape=sizes[2]), image_size=sizes[2])\n",
    "    s2 = level2.sample_ddim(lr=s1_r, x_T=level2.recon_noise, sampling_step_size=10)\n",
    "    s2_r = resize(s2, out_shape=sizes[3])\n",
    "    show_sample(torch.cat([s2, s2_ddpm, imgs[2]]), show_factor)\n",
    "    print(sifid(2, s2), sifid(2, s2_ddpm))\n",
    "\n",
    "    s3_ddpm = level3.sample(resize(s2_ddpm, out_shape=sizes[3]), image_size=sizes[3])\n",
    "    s3 = level3.sample_ddim(lr=s2_r, x_T=torch.randn_like(s2_r), sampling_step_size=10)\n",
    "    s3_r = resize(s3, out_shape=sizes[4])\n",
    "    show_sample(torch.cat([s3, s3_ddpm, imgs[3]]), show_factor)\n",
    "    print(sifid(3, s3), sifid(3, s3_ddpm))\n",
    "\n",
    "    s4_ddpm = level4.sample(resize(s3_ddpm, out_shape=sizes[4]), image_size=sizes[4])\n",
    "    s4 = level4.sample_ddim(lr=s3_r, x_T=torch.randn_like(s3_r), sampling_step_size=10)\n",
    "    show_sample(torch.cat([s4, s4_ddpm, imgs[4]]), show_factor)\n",
    "    print(sifid(4, s4), sifid(4, s4_ddpm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sanity check for recon-loss trained model. Using the recon_noise as initial sampling x_T, the results should be fairly similar to original image.\n",
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/version_1/checkpoints'\n",
    "\n",
    "show_factor = 15 # Just some constant for the show_sample func\n",
    "image_name = 'balloons.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "batch = 1\n",
    "\n",
    "sizes = [(186, 248)]\n",
    "imgs = [(imread(f'./images/{image_name}').to(device=device) * 2) - 1]\n",
    "\n",
    "for i in range(1, levels):\n",
    "    imgs.insert(0, resize(imgs[0], scale_factors=size_ratios))\n",
    "    sizes.insert(0, imgs[0].shape[-2:])\n",
    "    \n",
    "sifid = lambda i,s: get_sifid_scores((imgs[i].clamp(-1, 1) + 1) / 2, (s.clamp(-1, 1).unsqueeze(1) + 1) / 2)\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), \n",
    "                                                model=NextNet(), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "level1 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=1-step=29999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=9), timesteps=500, recon_loss_factor=1, recon_image=imgs[1], recon_image_lr=resize(imgs[0], out_shape=imgs[1].shape)\n",
    "                                               ).to(device='cuda:0')\n",
    "\n",
    "level2 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=2-step=29999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=9), timesteps=500, recon_loss_factor=1, recon_image=imgs[2], recon_image_lr=resize(imgs[1], out_shape=imgs[2].shape)\n",
    "                                               ).to(device='cuda:0')\n",
    "\n",
    "level3 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=3-step=29999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=9), timesteps=500, recon_loss_factor=1, recon_image=imgs[3], recon_image_lr=resize(imgs[2], out_shape=imgs[3].shape)\n",
    "                                               ).to(device='cuda:0')\n",
    "\n",
    "level4 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=4-step=29999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=9), timesteps=500, recon_loss_factor=1, recon_image=imgs[4], recon_image_lr=resize(imgs[3], out_shape=imgs[4].shape)\n",
    "                                               ).to(device='cuda:0')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # The next line generates the approximation to imgs[0] \n",
    "    # s0 = level0.sample_ddim(x_T=level0.recon_noise, sampling_step_size=50)\n",
    "    # Each of the next two lines generate a random level0 sample, instead of approximation imgs[0]\n",
    "    #s0 = level0.sample(image_size=sizes[0], batch_size=1)\n",
    "    s0 = level0.sample_ddim(x_T=torch.randn_like(level0.recon_noise), sampling_step_size=50)\n",
    "    s0_r = resize(s0, out_shape=sizes[1])\n",
    "    show_sample(torch.cat([s0, imgs[0]]), show_factor)\n",
    "    print(sifid(0, s0))\n",
    "    \n",
    "    s1_ddpm = level1.sample(resize(s0, out_shape=sizes[1]))\n",
    "    s1 = level1.sample_ddim(lr=s0_r, x_T=level1.recon_noise, sampling_step_size=10)\n",
    "    # s1 = level1.sample_ddim(lr=s0_r, x_T=torch.randn_like(level1.recon_noise), sampling_step_size=50)\n",
    "    s1_r = resize(s1, out_shape=sizes[2])\n",
    "    show_sample(torch.cat([s1, s1_ddpm, imgs[1]]), show_factor)\n",
    "    print(sifid(1, s1), sifid(1, s1_ddpm))\n",
    "    \n",
    "    s2_ddpm = level2.sample(resize(s1_ddpm, out_shape=sizes[2]))\n",
    "    s2 = level2.sample_ddim(lr=s1_r, x_T=level2.recon_noise, sampling_step_size=10)\n",
    "    # s2 = level2.sample_ddim(lr=s1_r, x_T=torch.randn_like(level2.recon_noise), sampling_step_size=50)\n",
    "    s2_r = resize(s2, out_shape=sizes[3])\n",
    "    show_sample(torch.cat([s2, s2_ddpm, imgs[2]]), show_factor)\n",
    "    print(sifid(2, s2), sifid(2, s2_ddpm))\n",
    "    \n",
    "    s3_ddpm = level3.sample(resize(s2_ddpm, out_shape=sizes[3]))\n",
    "    s3 = level3.sample_ddim(lr=s2_r, x_T=level3.recon_noise, sampling_step_size=10)\n",
    "    # s3 = level3.sample_ddim(lr=s2_r, x_T=torch.randn_like(level3.recon_noise), sampling_step_size=50)\n",
    "    s3_r = resize(s3, out_shape=sizes[4])\n",
    "    show_sample(torch.cat([s3, s3_ddpm, imgs[3]]), show_factor)\n",
    "    print(sifid(3, s3), sifid(3, s3_ddpm))\n",
    "    \n",
    "    s4_ddpm = level4.sample(resize(s3_ddpm, out_shape=sizes[4]))\n",
    "    s4 = level4.sample_ddim(lr=s3_r, x_T=level4.recon_noise, sampling_step_size=10)\n",
    "    # s4 = level4.sample_ddim(lr=s3_r, x_T=torch.randn_like(level4.recon_noise), sampling_step_size=50)\n",
    "    show_sample(torch.cat([s4, s4_ddpm, imgs[4]]), show_factor)\n",
    "    print(sifid(4, s4), sifid(4, s4_ddpm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To compare the previous cell results to old results of model trained without recon loss\n",
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/57_nextnet_all_layers-500-ts-much-faster-sampling/checkpoints'\n",
    "\n",
    "show_factor = 15 # Just some constant for the show_sample func\n",
    "image_name = 'balloons.png'\n",
    "levels = 5\n",
    "coarsest_size_ratio = 0.135\n",
    "size_ratios = coarsest_size_ratio ** (1.0 / (levels - 1))\n",
    "batch = 1\n",
    "\n",
    "sizes = [(186, 248)]\n",
    "imgs = [(imread(f'./images/{image_name}').to(device=device) * 2) - 1]\n",
    "\n",
    "for i in range(1, levels):\n",
    "    imgs.insert(0, resize(imgs[0], scale_factors=size_ratios))\n",
    "    sizes.insert(0, imgs[0].shape[-2:])\n",
    "    \n",
    "sifid = lambda i,s: get_sifid_scores((imgs[i].clamp(-1, 1) + 1) / 2, (s.clamp(-1, 1).unsqueeze(1) + 1) / 2)\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), \n",
    "                                                model=NextNet(), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "level1 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=1-step=34999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "level2 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=2-step=29999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "level3 = TheirsSRDiffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=3-step=29999.ckpt'),\n",
    "                                          model=NextNet(in_channels=6, depth=8), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use same s0 as recon-ddim model\n",
    "    show_sample(torch.cat([s0, imgs[0]]), show_factor)\n",
    "    print(sifid(0, s0))\n",
    "    \n",
    "    #s1_ddpm_normal = level1.sample(resize(s0, out_shape=sizes[1]))\n",
    "    s1_ddpm_normal = level1.sample(resize(imgs[0], out_shape=sizes[1]))\n",
    "    s1_r_normal = resize(s1_ddpm_normal, out_shape=sizes[2])\n",
    "    show_sample(torch.cat([s1_ddpm_normal, s1, s1_ddpm]), show_factor)\n",
    "    print(sifid(1, s1_ddpm_normal), sifid(1, s1), sifid(1, s1_ddpm))\n",
    "    \n",
    "    s2_ddpm_normal = level2.sample(resize(s1_ddpm_normal, out_shape=sizes[2]))\n",
    "    s2_r_normal = resize(s2_ddpm_normal, out_shape=sizes[3])\n",
    "    show_sample(torch.cat([s2_ddpm_normal, s2, s2_ddpm]), show_factor)\n",
    "    print(sifid(2, s2_ddpm_normal), sifid(2, s2), sifid(2, s2_ddpm))\n",
    "    \n",
    "    s3_ddpm_normal = level3.sample(resize(s2_ddpm_normal, out_shape=sizes[3]))\n",
    "    s3_r_normal = resize(s3_ddpm_normal, out_shape=sizes[4])\n",
    "    show_sample(torch.cat([s3_ddpm_normal, s3, s3_ddpm]), show_factor)\n",
    "    print(sifid(3, s3_ddpm_normal), sifid(3, s3), sifid(3, s3_ddpm))\n",
    "    \n",
    "    s4_ddpm_normal = level4.sample(resize(s3_ddpm_normal, out_shape=sizes[4]))\n",
    "    show_sample(torch.cat([imgs[4], s4_ddpm_normal, s4, s4_ddpm]), show_factor)\n",
    "    print(sifid(4, s4_ddpm_normal), sifid(4, s4), sifid(4, s4_ddpm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Method - CCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Experiment - Basic NextNet sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_factor = 15\n",
    "image_name = 'balloons.png'\n",
    "batch = 4\n",
    "img = imread(f'./images/{image_name}').to(device='cuda:0')\n",
    "\n",
    "get_sifid = lambda s: get_sifid_scores((img.clamp(-1, 1) + 1) / 2, (s.clamp(-1, 1).unsqueeze(1) + 1) / 2)\n",
    "\n",
    "model = Diffusion.load_from_checkpoint('/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/balloons.png/2-ccg-32-...256...-32-filters-128-crops-nextnet/checkpoints/single-level-step=29999.ckpt', \n",
    "                                        model=NextNet(filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "samples = model.sample(image_size=(64, 64), batch_size=batch)\n",
    "show_sample(samples)\n",
    "print(get_sifid(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    #ddim_samples = model.sample_ddim(image_size=(64, 64), batch_size=batch, sampling_step_size=100)\n",
    "    #show_sample(ddim_samples)\n",
    "    #ddim_samples = model.sample_ddim(image_size=(64, 64), batch_size=batch, sampling_step_size=50)\n",
    "    #show_sample(ddim_samples)\n",
    "    ddim_samples = model.sample_ddim(image_size=(256, 256), batch_size=batch, sampling_step_size=10)\n",
    "    show_sample(ddim_samples)\n",
    "    ddim_samples = model.sample_ddim(image_size=(64, 64), batch_size=batch, sampling_step_size=5)\n",
    "    show_sample(ddim_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Attempt - Single crop conditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_factor = 15\n",
    "image_name = 'balloons.png'\n",
    "batch = 16\n",
    "cs = (128, 128)\n",
    "img = imread(f'./images/{image_name}').to(device='cuda:0')\n",
    "\n",
    "get_sifid = lambda s: get_sifid_scores((img.clamp(-1, 1) + 1) / 2, (s.clamp(-1, 1).unsqueeze(1) + 1) / 2)\n",
    "\n",
    "model = TheirsSRDiffusion.load_from_checkpoint('/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/balloons.png/6-same-as-before-100k-training_steps/checkpoints/single-level-step=99999.ckpt', \n",
    "                                       model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "from torchvision import transforms\n",
    "from datasets.transforms import RandomScaleResize\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandomScaleResize(),\n",
    "        transforms.RandomCrop(cs, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.Lambda(lambda img: (img[:3, ] * 2) - 1)])\n",
    "\n",
    "def get_half_noisy_crop(img):\n",
    "    crop = torch.cat([transform(img) for i in range(batch)])\n",
    "    h_noise_index = int(-0.5 * cs[0])\n",
    "    crop[:, :, h_noise_index:, :] = torch.randn_like(crop[:, :, h_noise_index:, :])\n",
    "    return crop\n",
    "\n",
    "def combine_sample_and_original(sample, original):\n",
    "    assert sample.shape == original.shape\n",
    "    s = sample.clone()\n",
    "    h_noise_index = int(-0.5 * cs[0])\n",
    "    s[:, :, :h_noise_index, :] = original[:, :, :h_noise_index, :]\n",
    "    return s\n",
    "\n",
    "missing_sample = get_half_noisy_crop(img)\n",
    "show_sample(missing_sample)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #samples = model.sample_ddim(lr=missing_sample, sampling_step_size=10)\n",
    "    #show_sample(samples)\n",
    "    samples = model.sample(lr=missing_sample)\n",
    "    show_sample(samples)\n",
    "    samples = combine_sample_and_original(samples, missing_sample)\n",
    "    show_sample(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "missing_sample = torch.randn_like(missing_sample)\n",
    "show_sample(missing_sample)\n",
    "with torch.no_grad():\n",
    "    samples = model.sample(lr=missing_sample)\n",
    "    show_sample(samples)\n",
    "    samples = combine_sample_and_original(samples, missing_sample)\n",
    "    show_sample(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Attempt - CCG full sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = TheirsSRDiffusion.load_from_checkpoint('/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/balloons.png/6-same-as-before-100k-training_steps/checkpoints/single-level-step=99999.ckpt', \n",
    "                                       model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "cs = (128, 128)\n",
    "\n",
    "padding_size = (cs[0] // 4, cs[1] // 4)\n",
    "window_size = cs\n",
    "stride = (int(cs[0] * 0.9), int(cs[1] * 0.9))\n",
    "samples = model.sample_ccg(sample_size=(256, 256), batch_size=1, window_size=window_size, stride=stride, padding_size=padding_size, method='normal')\n",
    "show_sample(samples, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Experiment - Harmonization with CCG-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = TheirsSRDiffusion.load_from_checkpoint('/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/starry_night.png/0-ccg-noise-only-in-specific-locations/checkpoints/single-level-step=49999.ckpt', \n",
    "                                       model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "for t in range(80, 180, 10):\n",
    "    # Add noise only to non-transparent parts of addition_img\n",
    "    #t = 140\n",
    "    print(t)\n",
    "    addition_img = (resize(imread(r'./images/transparent-red-balloon.png', mode='RGBA').to(device=device), scale_factors=0.11) * 2) - 1\n",
    "    #addition_img = (resize(imread(r'./images/transparent-balloon.png', mode='RGBA').to(device=device), scale_factors=0.1) * 2) - 1\n",
    "    #addition_img = (resize(imread(r'./images/transparent-airplane.png', mode='RGBA').to(device=device), scale_factors=0.1) * 2) - 1\n",
    "    main_img = (imread('./images/starry_night.png').to(device=device) * 2) - 1\n",
    "    #main_img = (imread('./images/balloons.png').to(device=device) * 2) - 1\n",
    "    continuous_sqrt_alpha_cumprod = (torch.FloatTensor(np.random.uniform(model.sqrt_alphas_cumprod_prev[t - 1], model.sqrt_alphas_cumprod_prev[t], size=1)).to(device)).view(1, -1)\n",
    "    addition_img_noisy = model.q_sample(x_start=addition_img[:, :3, :, :], continuous_sqrt_alpha_cumprod=continuous_sqrt_alpha_cumprod.view(-1, 1, 1, 1))\n",
    "    addition_img = torch.cat([addition_img_noisy[0], (addition_img[0][3].unsqueeze(0) + 1) / 2])\n",
    "  \n",
    "    img = combine_images(main_img[0], addition_img, (25, 100)).unsqueeze(0)\n",
    "    torchvision.utils.save_image((img + 1) / 2, f'/home/yanivni/data/tmp/combined_starry_night_red_balloon.png')\n",
    "    with torch.no_grad():\n",
    "        #show_sample(starry_night)\n",
    "        #print('Noisy image')\n",
    "        #show_sample(img)\n",
    "        #print('DDIM sample starting from noisy img')\n",
    "        samples = model.sample_ddim(lr=img, sampling_step_size=10)\n",
    "        torchvision.utils.save_image((samples + 1) / 2, f'/home/yanivni/data/tmp/harmonization_starry_night_balloon_t={t}_ddim.png')\n",
    "        show_sample(samples)\n",
    "        print('DDPM sample starting from noisy img')\n",
    "        samples = model.sample(lr=img)\n",
    "        torchvision.utils.save_image((samples + 1) / 2, f'/home/yanivni/data/tmp/harmonization_starry_night_balloon_t={t}_ddpm.png')\n",
    "        show_sample(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Experiment - Style transfer with CCG-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model trained on starry night, sampled on balloons\n",
    "content_image_name = 'buildings.jpg'\n",
    "style_image_name = 'starry_night.png'\n",
    "version_name = '1-baseline-ccg'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{style_image_name}/{version_name}/checkpoints/single-level-step=49999.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "#x_T = torch.randn(size=(batch, 3, 200, 200), device=device)\n",
    "#samples = model.sample(x_T)\n",
    "#show_sample(samples)\n",
    "\n",
    "img = (imread(f'./images/{style_image_name}').to(device=device) * 2) - 1\n",
    "show_sample(img)\n",
    "img = (imread(f'./images/{content_image_name}').to(device=device) * 2) - 1\n",
    "show_sample(img)\n",
    "\n",
    "for t in range(150, 220, 20):\n",
    "    print(t)\n",
    "    t_vector = (torch.FloatTensor(np.random.uniform(model.sqrt_alphas_hat_prev[t - 1], model.sqrt_alphas_hat_prev[t], size=1)).to(device)).view(1, -1)\n",
    "    img_noisy = model.q_sample(img, t_vector.view(-1, 1, 1, 1))\n",
    "    with torch.no_grad():\n",
    "        samples = model.sample_ddim(condition=img_noisy, sampling_step_size=10)\n",
    "        show_sample(samples)\n",
    "           \n",
    "        samples = model.sample(condition=img_noisy)\n",
    "        show_sample(samples)\n",
    "        \n",
    "        sample_directory = os.path.join(saved_data_path, 'Style Transfer', f'{style_image_name}/{content_image_name}')\n",
    "        os.makedirs(sample_directory, exist_ok=True)\n",
    "        save_diffusion_sample(samples, os.path.join(sample_directory, f't={t}_sample.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Output generation and preparation for meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_names = ['penguins.jpg']\n",
    "\n",
    "for image_name in image_names:\n",
    "    version_name = '1-baseline-simple'\n",
    "    path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/last.ckpt'\n",
    "    saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "    model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16), timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "    # Generate images from complete noise in several sizes (Diverse Generation)\n",
    "    sizes = [tuple(imread(f'./images/{image_name}').shape[-2:])]\n",
    "    sizes.append((sizes[0][0] * 2, sizes[0][1]))\n",
    "    sizes.append((sizes[0][0], sizes[0][1] * 2))\n",
    "    sizes.append((sizes[0][0] * 2, sizes[0][1] * 2))\n",
    "\n",
    "    batch = 9\n",
    "    for size in sizes:\n",
    "        with torch.no_grad():\n",
    "            samples = model.sample(image_size=size, batch_size=batch)\n",
    "            show_sample(samples)\n",
    "        \n",
    "            sample_directory = os.path.join(saved_data_path, 'Diverse Generation', f'{image_name}/{size[0]}x{size[1]}')\n",
    "            os.makedirs(sample_directory, exist_ok=True)\n",
    "            save_diffusion_sample(samples, os.path.join(sample_directory, 'sample.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inpainting\n",
    "\n",
    "image_name = 'starry_night.png'\n",
    "version_name = '3-baseline-ccg-full-training-small-net'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/last.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "#model = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, depth=8), timesteps=500).to(device='cuda:0')\n",
    "   \n",
    "def add_noise_block_to_image(img, block_size, model, t_vector):\n",
    "    noise_mask = torch.zeros_like(img)\n",
    "    img_h, img_w = img.shape[-2:]\n",
    "    crop_start_loc = (random.randint(0, img_h - block_size[0] - 1), random.randint(1, img_h - block_size[1] - 1))\n",
    "    #img[:, :, crop_start_loc[0]:crop_start_loc[0] + block_size[0], crop_start_loc[1]:crop_start_loc[1] + block_size[1]].normal_()\n",
    "    \n",
    "    # TODO: This performs a certain \"dilation\" on the noise map\n",
    "    #noise_mask[:, :, max(0, crop_start_loc[0]-block_size[0]):crop_start_loc[0] + block_size[0]*2, max(0, crop_start_loc[1]-block_size[1]):crop_start_loc[1] + block_size[1]*2].fill_(0.4)\n",
    "    #noise_mask[:, :, max(0, crop_start_loc[0]-int(0.5*block_size[0])):crop_start_loc[0] + int(block_size[0]*1.5), max(0, crop_start_loc[1]-int(block_size[0]*0.5)):crop_start_loc[1] + int(block_size[1]*1.5)].fill_(0.7)\n",
    "    #noise_mask[:, :, crop_start_loc[0]:crop_start_loc[0] + block_size[0], crop_start_loc[1]:crop_start_loc[1] + block_size[1]].fill_(1)\n",
    "    #img = torch.randn_like(img) * noise_mask + img * (1 - noise_mask)\n",
    "    \n",
    "    noised_img = img.clone()\n",
    "    cur_t_img = noise_img(img, model, t_vector[0])\n",
    "    noised_img[:, :, max(0, crop_start_loc[0]-block_size[0]):crop_start_loc[0] + block_size[0]*2, max(0, crop_start_loc[1]-block_size[1]):crop_start_loc[1] + block_size[1]*2] = \\\n",
    "                cur_t_img[:, :, max(0, crop_start_loc[0]-block_size[0]):crop_start_loc[0] + block_size[0]*2, max(0, crop_start_loc[1]-block_size[1]):crop_start_loc[1] + block_size[1]*2]\n",
    "    cur_t_img = noise_img(img, model, t_vector[1])\n",
    "    noised_img[:, :, max(0, crop_start_loc[0]-int(0.5*block_size[0])):crop_start_loc[0] + int(block_size[0]*1.5), max(0, crop_start_loc[1]-int(block_size[0]*0.5)):crop_start_loc[1] + int(block_size[1]*1.5)] = \\\n",
    "                cur_t_img[:, :, max(0, crop_start_loc[0]-int(0.5*block_size[0])):crop_start_loc[0] + int(block_size[0]*1.5), max(0, crop_start_loc[1]-int(block_size[0]*0.5)):crop_start_loc[1] + int(block_size[1]*1.5)] \n",
    "    cur_t_img = noise_img(img, model, t_vector[2])\n",
    "    noised_img[:, :, crop_start_loc[0]:crop_start_loc[0] + block_size[0], crop_start_loc[1]:crop_start_loc[1] + block_size[1]] = \\\n",
    "                cur_t_img[:, :, crop_start_loc[0]:crop_start_loc[0] + block_size[0], crop_start_loc[1]:crop_start_loc[1] + block_size[1]]\n",
    "    img = noised_img\n",
    "    \n",
    "    return img\n",
    "    \n",
    "# Generate some sample conditional crops (Inpainting)\n",
    "inpainting_crop_sizes = [(32,32), (64, 64), (64, 128), (128, 64), (140, 140)]\n",
    "batch = 4\n",
    "for size in inpainting_crop_sizes:\n",
    "    # Add noise to specific crop in image\n",
    "    img = ((imread(fr'./images/{image_name}') * 2) - 1).to(device=device)\n",
    "    img = add_noise_block_to_image(img, size, model, (150, 300, 500))\n",
    "    show_sample(img)\n",
    "    sample_directory = os.path.join(saved_data_path, 'Inpainting', 'non-binary-mask', f'{image_name}/{size[0]}x{size[1]}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    save_diffusion_sample(img, os.path.join(sample_directory, 'noised_image.png'))\n",
    "    \n",
    "    for i in range(batch):\n",
    "        sample = model.sample(condition=img)\n",
    "        show_sample(sample)\n",
    "        save_diffusion_sample(sample, os.path.join(sample_directory, f'{i}_sample.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Harmonize specific objects into an image\n",
    "image_name = 'seascape.png'\n",
    "version_name = '1-baseline-ccg'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=99999.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs'\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "\n",
    "additional_img_names_to_sizes_ratios = {\n",
    "    'transparent-red-balloon.png': 0.1,\n",
    "    'transparent-balloon.png': 0.1,\n",
    "    'transparent-spaceship.png': 0.05,\n",
    "    'transparent-airplane.png': 0.1,\n",
    "    'transparent-lizard.png': 0.08,\n",
    "}\n",
    "\n",
    "harmonization_locations = {\n",
    "    'starry_night.png': (25, 100),\n",
    "    'balloons.png': (0, 0),\n",
    "    'stone.png': (120, 110),\n",
    "    'seascape.png': (10, 200)\n",
    "}\n",
    "\n",
    "for additional_img_name, ratio in additional_img_names_to_sizes_ratios.items():\n",
    "    addition_img = (resize(imread(fr'./images/{additional_img_name}', mode='RGBA').to(device=device), scale_factors=ratio) * 2) - 1\n",
    "    main_img = (imread(f'./images/{image_name}').to(device=device) * 2) - 1\n",
    "    \n",
    "    sample_directory = os.path.join(saved_data_path, 'Harmonization', f'{image_name}/{additional_img_name}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    \n",
    "    dummy_additional_img = torch.cat([addition_img[0][:3], (addition_img[0][3].unsqueeze(0) + 1) / 2])\n",
    "    dummy_img = combine_images(main_img[0], dummy_additional_img, harmonization_locations[image_name]).unsqueeze(0)\n",
    "    show_sample(dummy_img, 15)\n",
    "    save_diffusion_sample(dummy_img, os.path.join(sample_directory, 'before_harmonization.png'))\n",
    "    \n",
    "    for t in range(1, 501, 50):\n",
    "        print(t)\n",
    "        # Add noise only to non-transparent parts of addition_img\n",
    "        t_vector = (torch.FloatTensor(np.random.uniform(model.sqrt_alphas_hat_prev[t - 1], model.sqrt_alphas_hat_prev[t], size=1)).to(device)).view(1, -1)\n",
    "        addition_img_noisy = addition_img[:, :3, :, :]#model.q_sample(x_start=addition_img[:, :3, :, :], continuous_sqrt_alpha_hat=t_vector.view(-1, 1, 1, 1))\n",
    "        addition_img_concat = torch.cat([addition_img_noisy[0], (addition_img[0][3].unsqueeze(0) + 1) / 2])\n",
    "        img = combine_images(main_img[0], addition_img_concat, harmonization_locations[image_name]).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            samples = model.sample_ddim(condition=img, sampling_step_size=10)\n",
    "            save_diffusion_sample(samples, os.path.join(sample_directory, f't={t}_ddim.png'))\n",
    "            show_sample(samples, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conditional inpainting (Color-guided inpainting)\n",
    "image_name = 'fruit.png'\n",
    "version_name = '2-baseline-ccg-full-training-normal-net'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/last.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs'\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "colors = {\n",
    "    'yellow': torch.tensor([[[1,1,0]]], dtype=torch.float32, device=device),\n",
    "    'red': torch.tensor([[[1,0,0]]], dtype=torch.float32, device=device),\n",
    "    'green': torch.tensor([[[0,1,0]]], dtype=torch.float32, device=device),\n",
    "    'blue': torch.tensor([[[0,0,1]]], dtype=torch.float32, device=device)\n",
    "}\n",
    "\n",
    "# Add noise to specific crop in image\n",
    "img = ((imread(fr'./images/{image_name}') * 2) - 1).to(device=device)\n",
    "crop_start_loc = (74, 115)\n",
    "#crop_size = (26, 52)\n",
    "crop_size = (50, 110)\n",
    "\n",
    "batch = 4\n",
    "for color_name, color in colors.items():\n",
    "    color_tensor = color.repeat((crop_size[0], crop_size[1], 1)).moveaxis(2, 0)\n",
    "    \n",
    "    #t = 250\n",
    "    #t_tensor = torch.cuda.FloatTensor(np.random.uniform(model.sqrt_alphas_hat_prev[t - 1], model.sqrt_alphas_hat_prev[t], size=1)).to(device='cuda:0')\n",
    "    #noisy_color_tensor = model.q_sample(x_start=color_tensor, continuous_sqrt_alpha_hat=t_tensor.view(-1, 1, 1, 1))  \n",
    "    noisy_color_tensor = color_tensor + torch.randn_like(color_tensor)\n",
    "    img[:, :, crop_start_loc[0]:crop_start_loc[0] + crop_size[0], crop_start_loc[1]:crop_start_loc[1] + crop_size[1]] = noisy_color_tensor\n",
    "    show_sample(img)\n",
    "    sample_directory = os.path.join(saved_data_path, 'Conditional Inpainting', f'{image_name}/{color_name}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    save_diffusion_sample(img, os.path.join(sample_directory, f'noised_image.png'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(batch):\n",
    "            sample = model.sample(condition=img)\n",
    "            show_sample(sample)\n",
    "            save_diffusion_sample(sample, os.path.join(sample_directory, f'{i}_sample.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Image Editing\n",
    "\n",
    "image_name = 'stone.png'\n",
    "edit_name = 'stone_edit.png'\n",
    "version_name = '3-baseline-simple'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/last.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16), timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "edit_img = imread(f'./images/edit/{edit_name}').to(device=device) * 2 - 1\n",
    "for t in range(100, 350, 20):\n",
    "    print(t)\n",
    "    noisy_edit_img = noise_img(edit_img, model, t)\n",
    "    show_sample(noisy_edit_img)\n",
    "    samples = model.sample(custom_initial_img=noisy_edit_img, custom_timesteps=t)\n",
    "    show_sample(samples)\n",
    "        \n",
    "    sample_directory = os.path.join(saved_data_path, 'Editing', f'{image_name}/{edit_name}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    save_diffusion_sample(edit_img, os.path.join(sample_directory, 'original_edit.png'))\n",
    "    save_diffusion_sample(samples, os.path.join(sample_directory, f'sample_t={t}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visual Summary\n",
    "\n",
    "image_name = 'birds_3.jpg'\n",
    "version_name = '2-baseline-vs'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/last.ckpt'\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=11, filters_per_layer=[32, 64, 64, 128, 128, 256, 128, 128, 64, 64, 32]), timesteps=500, strict=False).to(device='cuda:0')\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs'\n",
    "\n",
    "    \n",
    "def summarize_image(img, model, n_iteration=10, scale=(0.9, 0.9), t=100):\n",
    "    resized_imgs = [img]\n",
    "    for i in range(n_iterations):\n",
    "        resized_img = resize(resized_imgs[-1], scale_factors=scale)\n",
    "        fixed_img = model.sample(custom_initial_img = noise_img(resized_img, model, t), custom_timesteps=t)\n",
    "        resized_imgs.append(fixed_img)\n",
    "        show_sample(fixed_img)\n",
    "    return resized_imgs\n",
    "\n",
    "img = imread(f'./images/{image_name}').to(device=device) * 2 - 1 \n",
    "final_size = {'tree.png': (img.shape[-2] // 4, img.shape[-1] // 4),\n",
    "              'balloons.png': (img.shape[-2] // 4, img.shape[-1] // 4),\n",
    "              'mountains3.png': (img.shape[-2] // 1.2, img.shape[-1] // 4),\n",
    "              'birds_3.jpg': (img.shape[-2] // 1.1, img.shape[-1] // 3)}[image_name]\n",
    "\n",
    "n_iterations = 15\n",
    "scale = ((final_size[0] / img.shape[-2]) ** (1 / n_iterations), (final_size[1] / img.shape[-1]) ** (1 / n_iterations))\n",
    "print(scale)\n",
    "\n",
    "for t in [100, 150, 250]:\n",
    "    print(f't={t}')\n",
    "    samples = summarize_image(img, model, n_iterations, scale, t)\n",
    "    sample_directory = os.path.join(saved_data_path, 'Visual Summary', f'{image_name}/t={t}_scale={scale[0] :2f}_{scale[1] :2f}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    \n",
    "    for i, s in enumerate(samples):\n",
    "        print(f'scale={(scale[0] ** i, scale[1] ** i)}')\n",
    "        show_sample(s)\n",
    "        save_diffusion_sample(s, os.path.join(sample_directory, f'scale={scale[0] ** i :2f}_{scale[1] ** i :2f}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Draw from sketch\n",
    "\n",
    "sketch_name = 'starry_night_sketch.png'\n",
    "image_name = 'starry_night.png'\n",
    "version_name = '2-simple-diffusion-huge-crops-nextnet-depth-16-part3'\n",
    "\n",
    "sketch_name = 'balloons_sketch_1.png'\n",
    "image_name = 'balloons.png'\n",
    "version_name = '10-simple-diffusion-huge-crops-nextnet-depth-16-part4'\n",
    "\n",
    "sketch_name = 'cows_sketch.png'\n",
    "image_name = 'cows.png'\n",
    "version_name = '1-baseline-simple'\n",
    "\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=199999.ckpt'\n",
    "sketch_model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16), timesteps=500, auto_sample=False, strict=False).to(device='cuda:0')\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "\n",
    "\n",
    "show_sample((imread(f'./images/{image_name}').to(device=device) * 2) - 1)\n",
    "sketch_img = (imread(f'./images/sketch/{sketch_name}').to(device=device) * 2) - 1\n",
    "show_sample(sketch_img)\n",
    "\n",
    "for t in range(240, 320, 20):\n",
    "    print(t)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_noisy = noise_img(sketch_img, sketch_model, t)\n",
    "        samples = sketch_model.sample(custom_initial_img=img_noisy, custom_timesteps=t, batch_size=1)\n",
    "        show_sample(samples)\n",
    "        \n",
    "    sample_directory = os.path.join(saved_data_path, 'Draw from sketch', f'{image_name}/{sketch_name}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    save_diffusion_sample(sketch_img, os.path.join(sample_directory, f'raw_sketch.png'))\n",
    "    save_diffusion_sample(samples, os.path.join(sample_directory, f't={t}_sample.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiment- Using ILVR sampling for better conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_name = 'balloons.png'\n",
    "version_name = '8-baseline-ccg'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=49999.ckpt'\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device=device)\n",
    "\n",
    "\n",
    "def phi_N(img, N):\n",
    "    out_size = (img.shape[-2], img.shape[-1])\n",
    "    return resize(resize(img, scale_factors=1/N), scale_factors=N, out_shape=out_size)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ilvr(model, reference_img, N=1):\n",
    "    b = reference_img.shape[0]\n",
    "    condition = torch.randn_like(reference_img)\n",
    "    img = torch.randn_like(reference_img)\n",
    "    for i in reversed(range(0, model.num_timesteps)):\n",
    "        if i > 0:\n",
    "            img_tag = model.p_sample(img, i, condition_x=condition)\n",
    "\n",
    "            continuous_sqrt_alpha_hat = torch.FloatTensor(\n",
    "                    np.random.uniform(model.sqrt_alphas_hat_prev[i - 1], model.sqrt_alphas_hat_prev[i], size=b)).to(model.device).view(b, -1)\n",
    "            reference_noised = model.q_sample(reference_img, continuous_sqrt_alpha_hat)\n",
    "\n",
    "            img = phi_N(reference_noised, N) + img_tag - phi_N(img_tag, N)\n",
    "        else:\n",
    "            img = model.p_sample(img, i, condition_x=condition)\n",
    "    return img\n",
    "\n",
    "    \n",
    "reference_img = ((imread(fr'./images/{image_name}') * 2) - 1).to(device=device)\n",
    "show_sample(reference_img)\n",
    "\n",
    "for N in [2, 4, 8, 16, 32, 64]:\n",
    "    print(f'N={N}')\n",
    "    samples = sample_ilvr(model, reference_img=reference_img, N=N)\n",
    "    show_sample(phi_N(reference_img, N))\n",
    "    show_sample(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combine ILVR with conditioning based on level0 generation from old pyramid model\n",
    "image_name = 'balloons.png'\n",
    "version_name = '6-same-as-before-100k-training_steps'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=99999.ckpt'\n",
    "model_big = TheirsSRDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device=device)\n",
    "\n",
    "image_name = 'balloons_medium.png'\n",
    "version_name = 'version_1'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=99999.ckpt'\n",
    "model_med = ConditionalDiffusion.load_from_checkpoint(path, model=NextNet(in_channels=6, filters_per_layer=[32, 64, 64, 128, 256, 128, 64, 64, 32]), timesteps=500).to(device=device)\n",
    "\n",
    "path_to_checkpoints = r'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/pyramid/balloons.png/version_4/checkpoints'\n",
    "image_name = 'balloons.png'\n",
    "batch = 1\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ilvr_2(model, reference_img, N=1):\n",
    "    b = reference_img.shape[0]\n",
    "    condition = torch.randn_like(reference_img)\n",
    "    img = torch.randn_like(reference_img)\n",
    "    for i in reversed(range(0, model.num_timesteps)):\n",
    "        if i > 0:\n",
    "            img_tag = model.p_sample(img, i, condition_x=condition)\n",
    "\n",
    "            continuous_sqrt_alpha_hat = torch.FloatTensor(\n",
    "                    np.random.uniform(model.sqrt_alphas_cumprod_prev[i - 1], model.sqrt_alphas_cumprod_prev[i], size=b)).to(model.device).view(b, -1)\n",
    "            reference_noised = model.q_sample(reference_img, continuous_sqrt_alpha_hat)\n",
    "\n",
    "            img = phi_N(reference_noised, N) + img_tag - phi_N(img_tag, N)\n",
    "        else:\n",
    "            img = model.p_sample(img, i, condition_x=condition)\n",
    "    return img\n",
    "\n",
    "\n",
    "imgs = [(imread(f'./images/balloons_medium.png').to(device=device) * 2) - 1, (imread(f'./images/balloons.png').to(device=device) * 2) - 1]\n",
    "\n",
    "level0 = Diffusion.load_from_checkpoint(os.path.join(path_to_checkpoints, 'level=0-step=29999.ckpt'), \n",
    "                                                model=NextNet(depth=12), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "\n",
    "level0_size = (186, 248)\n",
    "reference_img = level0.sample(image_size=level0_size, batch_size=1)\n",
    "show_sample(torch.cat([reference_img, resize(imgs[-1], out_shape=reference_img.shape)]), 15)\n",
    "save_diffusion_sample(reference_img, f'/home/yanivni/data/tmp/coarsest_scale_conditioning_experiment/0_coarsest_scale.png')\n",
    "\n",
    "reference_img = resize(reference_img, out_shape=imgs[0].shape)\n",
    "N = imgs[0].shape[-2] / level0_size[0]\n",
    "samples = sample_ilvr(model_med, reference_img, N)\n",
    "save_diffusion_sample(samples, f'/home/yanivni/data/tmp/coarsest_scale_conditioning_experiment/1_medium_scale_sample.png')\n",
    "show_sample(torch.cat([samples, resize(imgs[-1], out_shape=samples.shape)]), 15)\n",
    "\n",
    "samples = resize(samples, out_shape=imgs[1].shape)\n",
    "N = imgs[1].shape[-2] / imgs[0].shape[-2]\n",
    "samples = sample_ilvr_2(model_big, samples, N)\n",
    "show_sample(torch.cat([samples, resize(imgs[-1], out_shape=samples.shape)]), 15)\n",
    "save_diffusion_sample(samples, f'/home/yanivni/data/tmp/coarsest_scale_conditioning_experiment/2_fine_scale_sample.png')\n",
    "\n",
    "N = imgs[1].shape[-2] / level0_size[0]\n",
    "reference_img = resize(reference_img, out_shape=imgs[1].shape)\n",
    "samples = sample_ilvr_2(model_big, reference_img, N)\n",
    "show_sample(torch.cat([samples, resize(imgs[-1], out_shape=samples.shape)]), 15)\n",
    "save_diffusion_sample(samples, f'/home/yanivni/data/tmp/coarsest_scale_conditioning_experiment/3_direct_fine_scale_sample.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiment - More attempts at diverse generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from diffusion.diffusion import Diffusion\n",
    "\n",
    "image_name = 'starry_night.png'\n",
    "version_name = 'version_0'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=99999.ckpt' # +75K\n",
    "model1 = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "batch = 1\n",
    "sample_size = (200, 350)\n",
    "x_T = torch.randn(size=(batch, 3, sample_size[0], sample_size[1]), device=device)\n",
    "samples = model1.sample_ddim(x_T=x_T, sampling_step_size=10)\n",
    "show_sample(samples, 10)\n",
    "samples = model1.sample(image_size=sample_size, batch_size=batch)\n",
    "show_sample(samples, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiment - Playing around with latent space representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_name = 'balloons.png'\n",
    "version_name = '10-simple-diffusion-huge-crops-nextnet-depth-16-part4'#9.0-simple-diffusion-half-resolution-huge-crops-nextnet'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/last.ckpt'\n",
    "#model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=9), timesteps=500).to(device='cuda:0')\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16), timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "batch = 1\n",
    "sample_size = (186, 250)\n",
    "x_T = torch.randn(size=(batch, 3, sample_size[0], sample_size[1]), device=device)\n",
    "x_T2 = torch.randn(size=(batch, 3, sample_size[0], sample_size[1]), device=device)\n",
    "x_T = torch.load(f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/high_res/good_xT.pt')\n",
    "x_T2 = torch.load(f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/high_res/good_xT2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sss = 5 # sampling step size\n",
    "samples = model.sample_ddim(x_T=x_T, batch_size=1, sampling_step_size=sss)\n",
    "show_sample(samples)    \n",
    "samples = model.sample_ddim(x_T=x_T2, batch_size=1, sampling_step_size=sss)\n",
    "show_sample(samples)\n",
    "\n",
    "balloon_means = x_T2[:, :, 0:35, 75:100].mean(dim=(-2, -1)).view(1,3,1,1)  \n",
    "\n",
    "clean_xT2 = x_T2.clone()\n",
    "clean_xT2[:, :, :50, :].fill_(0)\n",
    "#clean_xT2[:, :, :50, :] = torch.randn_like(clean_xT2[:, :, :50, :]) + balloon_means\n",
    "samples = model.sample_ddim(x_T=clean_xT2, batch_size=1, sampling_step_size=10)\n",
    "show_sample(samples)\n",
    "#save_diffusion_sample(samples, f'/home/yanivni/data/tmp/latent_space_editing/changing_upper_part_of_image/positive_noise/{i}.png')\n",
    "        \n",
    "#clean_xT2[:, :, :45, :] = (torch.randn_like(clean_xT2[:, :, :45, :])) - 0.0438\n",
    "#samples = model.sample_ddim(x_T=clean_xT2, batch_size=1, sampling_step_size=10)\n",
    "#show_sample(samples)\n",
    "#save_diffusion_sample(samples, f'/home/yanivni/data/tmp/latent_space_editing/changing_upper_part_of_image/negative_noise/{i}.png')\n",
    "        \n",
    "samples = []\n",
    "#x_T2_chunk = x_T[:, :, :35, 45:70]\n",
    "clean_xT = x_T.clone()\n",
    "clean_xT[:, :, :150, :].fill_(0)\n",
    "x_T_chunk = x_T[:, :, 50:150, :]\n",
    "for i in range(0, 50):\n",
    "    x_T_patched = clean_xT.clone()\n",
    "    x_T_patched[:, :, 50-i:50-i+x_T_chunk.shape[-2], :x_T_chunk.shape[-1]] = x_T_chunk\n",
    "    samples.append(model.sample_ddim(x_T=x_T_patched, batch_size=1, sampling_step_size=sss))\n",
    "samples = torch.cat(samples, dim=0)\n",
    "show_gif(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run the above 10 times to generate gifs and understand if what we see is accidental or shows a cool phenomena\n",
    "gif_count = 10\n",
    "batch = 1\n",
    "out_dir = '/home/yanivni/data/tmp/latent_space_editing/balloons.png/medium_res/interp_gifs'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for gif_index in range(gif_count):\n",
    "        x_T = torch.randn((batch, 3, sample_size[0], sample_size[1]), device='cuda:0')\n",
    "        x_T2 = torch.randn((batch, 3, sample_size[0], sample_size[1]), device='cuda:0')\n",
    "        s0 = model.sample_ddim(x_T=x_T, sampling_step_size=10)\n",
    "        s0_2 = model.sample_ddim(x_T=x_T2, sampling_step_size=10)\n",
    "        \n",
    "        save_sample(s0, output_path=fr'{out_dir}/{gif_index}_a1.png')\n",
    "        save_sample(s0_2, output_path=fr'{out_dir}/{gif_index}_a2.png')\n",
    "        \n",
    "        # Interpolate\n",
    "        interp_size = 100\n",
    "        samples = []\n",
    "        for i in range(interp_size):\n",
    "            x_T_interp = x_T * (1 - (i / (interp_size - 1))) + x_T2 * (i / (interp_size - 1))\n",
    "            s_interp = model.sample_ddim(x_T=x_T_interp, sampling_step_size=10)\n",
    "            samples.append(s_interp.cpu())\n",
    "        samples = torch.cat(samples, dim=0)\n",
    "        make_gif(samples, f'{out_dir}/{gif_index}_gif.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(x_T, f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/good_xT.pt')\n",
    "torch.save(x_T2, f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/good_xT2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_name = 'birds.png'\n",
    "version_name = '2-simple-diffusion-huge-crops-nextnet'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=49999.ckpt'\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=12), timesteps=500).to(device='cuda:0')\n",
    "\n",
    "batch = 1\n",
    "sample_size = (200, 200)\n",
    "x_T = torch.randn(size=(batch, 3, sample_size[0], sample_size[1]), device=device)\n",
    "x_T2 = torch.randn(size=(batch, 3, sample_size[0], sample_size[1]), device=device)\n",
    "x_T = torch.load(f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/good_xT.pt')\n",
    "x_T2 = torch.load(f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/good_xT2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def small_change(x, m, v):\n",
    "    h_size = int(x.shape[-2] ** 0.4)\n",
    "    w_size = int(x.shape[-1] ** 0.4)\n",
    "    \n",
    "    h_loc, w_loc = random.randint(0, x.shape[-2] - h_size), random.randint(0, x.shape[-1] - w_size)\n",
    "    x[:, :, h_loc:h_loc + h_size, w_loc:w_loc + w_size] += m #torch.randn_like(x[:, :, h_loc:h_loc + h_size, w_loc:w_loc + w_size]) * torch.sqrt(v) + m\n",
    "    return x\n",
    "\n",
    "def chunk_interp(chunks, i, lim):\n",
    "    cur_idx_float = (i / lim) * (len(chunks)-1)\n",
    "    cur_idx = int(cur_idx_float)\n",
    "    alpha = cur_idx_float % 1\n",
    "    return (1 - alpha) * chunks[cur_idx] + alpha * chunks[cur_idx + 1]\n",
    "\n",
    "def rotate_interp(x, chunks, i, lim):\n",
    "    rotated_x = x.roll(i, dims=(-1))\n",
    "    \n",
    "    new_chunk = chunk_interp(chunks, i, lim)\n",
    "    dir_x = new_chunk - chunks[0]\n",
    "    dir_x = dir_x.repeat((1, 1, 7, 7))\n",
    "    \n",
    "    rotated_x = rotated_x + dir_x[:, :, :rotated_x.shape[-2], :rotated_x.shape[-1]]\n",
    "    return rotated_x\n",
    "    \n",
    "with torch.no_grad():\n",
    "    samples = model.sample_ddim(x_T=x_T, sampling_step_size=10)\n",
    "    show_sample(samples)    \n",
    "    samples = model.sample_ddim(x_T=x_T2, sampling_step_size=10)\n",
    "    show_sample(samples)\n",
    "    \n",
    "    clean_xT2 = x_T2.clone()\n",
    "    #clean_xT2[:, :, :, :].fill_(0)\n",
    "    samples = model.sample_ddim(x_T=clean_xT2, sampling_step_size=10)\n",
    "    \n",
    "    samples = []\n",
    "    chunks = [x_T[:, :, 80:110, 95:125], x_T2[:, :, 80:110, 70:100], x_T[:, :, 55:85, 40:70], x_T[:, :, 55:85, 120:150], x_T[:, :, 50:80, 85:115],\n",
    "              x_T[:, :, 80:110, 95:125], x_T2[:, :, 80:110, 70:100], x_T[:, :, 55:85, 40:70]]\n",
    "    #m, v = chunk.mean(), x_T2_chunk.var()\n",
    "    lim = 120\n",
    "    for i in range(0, lim):\n",
    "        x_T_patched = clean_xT2.clone()\n",
    "        #chunk = small_change(chunk, m, v)\n",
    "        chunk = chunk_interp(chunks, i, lim)\n",
    "        #x_T_patched[:, :, 20:20+chunk.shape[-2], i:i+chunk.shape[-1]] = chunk\n",
    "        x_T_patched[:, :, :chunk.shape[-2], i:i+chunk.shape[-1]] += (chunk - chunks[0])\n",
    "        x_T_patched[:, :, 30:30+chunk.shape[-2], i:i+chunk.shape[-1]] += (chunk - chunks[0])\n",
    "        x_T_patched[:, :, 60:60+chunk.shape[-2], i:i+chunk.shape[-1]] += (chunk - chunks[0])\n",
    "        #x_T_patched[:, :, :150, :] = rotate_interp(clean_xT2[:, :, :150, :], chunks, i, lim)\n",
    "        samples.append(model.sample_ddim(x_T=x_T_patched, sampling_step_size=10))\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "    show_gif(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(x_T, f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/good_xT.pt')\n",
    "torch.save(x_T2, f'/home/yanivni/data/tmp/latent_space_editing/{image_name}/good_xT2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - Generate video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling method 1 - generate frame0 from noise and then noise+denoise_with_next_frame to get the next frame\n",
    "video_name = 'dutch2'\n",
    "\n",
    "version_name = 'version_3'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{video_name}/{version_name}/checkpoints/last.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16, frame_conditioned=True), timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "size = tuple(imread(f'./images/video/{video_name}/1.png').shape[-2:])\n",
    "total_frame_count = len(os.listdir(f'./images/video/{video_name}'))\n",
    "\n",
    "t = 200\n",
    "s0 = model.sample(image_size=size, batch_size=1, frame=0)\n",
    "show_sample(s0)\n",
    "samples = [s0]\n",
    "for frame in range(1, total_frame_count + 1):\n",
    "    noisy_prev_frame = noise_img(samples[-1], model, t)\n",
    "    s = model.sample(custom_initial_img=noisy_prev_frame, custom_timesteps=t, frame=frame)\n",
    "    show_sample(s)\n",
    "    samples.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gif(torch.cat(samples, dim=0), interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling method 2 - Use an existing frame as frame0 and then noise+denoise_with_next_frame to get the next frame\n",
    "video_name = 'birds4'\n",
    "\n",
    "version_name = 'version_0'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{video_name}/{version_name}/checkpoints/last.ckpt'\n",
    "\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path,\n",
    "                                              model=NextNet(in_channels=6, depth=16, frame_conditioned=True),\n",
    "                                              timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "total_frame_count = len(os.listdir(f'./images/video/{video_name}'))\n",
    "print(total_frame_count)\n",
    "\n",
    "s0 = imread(f'./images/video/{video_name}/1.png').to(device=device) * 2 - 1\n",
    "show_sample(s0)\n",
    "samples = [s0]\n",
    "for frame in range(1, total_frame_count + 1):\n",
    "    print(frame, )\n",
    "    s = model.sample(condition=samples[-1], frame=1)\n",
    "    show_sample(s)\n",
    "    samples.append(s)\n",
    "#show_gif(torch.cat(samples, dim=0), interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gif(torch.cat(samples, dim=0), interval=100)\n",
    "#torchvid2mp4(resize(torch.cat(samples, dim=0), out_shape=(3,(size[0]//2)*2,(size[1]//2)*2)).permute((1, 0, 2, 3)), f'/home/yanivni/data/tmp/video_generation/{video_name}/sample_294k.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_samples = []\n",
    "#total_frame_count = \n",
    "for frame in range(1, len(os.listdir(f'./images/video/{video_name}')) + 1):\n",
    "    gt_samples.append(imread(f'./images/video/{video_name}/{frame}.png').to(device=device) * 2 - 1)\n",
    "show_gif(torch.cat(gt_samples, dim=0), interval=100)\n",
    "torchvid2mp4(resize(torch.cat(gt_samples, dim=0), out_shape=(3,(size[0]//2)*2,(size[1]//2)*2)).permute((1, 0, 2, 3)), f'/home/yanivni/data/tmp/video_generation/{video_name}/gt.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html_results.html_utils import create_results_html\n",
    "\n",
    "base_dir = '/home/yanivni/data/tmp/organized-outputs/Video Generation/ski_slope'\n",
    "html_filename = 'result.html'\n",
    "vid_folders = [\n",
    "#     (f'scale_{s}', [f'{s}/v', f'{s}/q', f'{s}/r'], 'green', 'flex', 30, True) for s in reversed(range(VGPNN.n_stages))\n",
    "    (f'bla', [f'{i}' for i in range(1, 5)], 'green', 'flex', 30, True),\n",
    "]\n",
    "create_results_html(vid_folders, base_dir, html_filename, frame_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling method 3 - Use an existing frame as frame0, generate every N'th frame from it and interpolate the rest of the\n",
    "# frames (conditional on both 0 and N'th frame)\n",
    "\n",
    "video_name = 'air_balloons'\n",
    "version_name = '6-all-frames-frame-diff-embedding-also-negative'\n",
    "interp_version_name = '7-all-frames-conditioned-on-past-and-future-frames'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{video_name}/{version_name}/checkpoints/last.ckpt'\n",
    "interp_path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{video_name}/{interp_version_name}/checkpoints/last.ckpt'\n",
    "saved_data_path = f'/home/yanivni/data/tmp/organized-outputs/'\n",
    "\n",
    "model = ConditionalDiffusion.load_from_checkpoint(path, \n",
    "                                                  model=NextNet(in_channels=6, depth=16, frame_conditioned=True),\n",
    "                                                  timesteps=500, strict=False).to(device='cuda:0')\n",
    "interp_model = ConditionalDiffusion.load_from_checkpoint(interp_path, \n",
    "                                                         model=NextNet(in_channels=9, depth=16, frame_conditioned=True),\n",
    "                                                         timesteps=500, strict=False).to(device='cuda:0')\n",
    "\n",
    "total_frame_count = 100#len(os.listdir(f'./images/video/{video_name}'))\n",
    "s0 = imread(f'./images/video/{video_name}/1.png').to(device=device) * 2 - 1\n",
    "show_sample(s0)\n",
    "samples = [s0]\n",
    "N = 2\n",
    "\n",
    "for frame in range(0, total_frame_count + 1, N):\n",
    "    s = model.sample(condition=samples[-1], frame=N)\n",
    "    condition_interp = torch.cat((samples[-1], s), dim=1)\n",
    "    for interp_frame in range(1, N):\n",
    "        print(frame + interp_frame)\n",
    "        s_interp = interp_model.sample(condition=condition_interp, frame=(interp_frame, N - interp_frame))\n",
    "        show_sample(s_interp)\n",
    "        samples.append(s_interp)\n",
    "    print(frame + N)\n",
    "    show_sample(s)\n",
    "    samples.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_gif(torch.cat(samples, dim=0), interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiment - Perform DG based on VS \"pyramid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_name = 'balloons.png'\n",
    "version_name = '11-simple-diffusion-64-crops-nextnet-depth-11-btlnk-for-vs'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=199999.ckpt'\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=11, filters_per_layer=[32, 64, 64, 128, 128, 256, 128, 128, 64, 64, 32]), timesteps=500, strict=False).to(device='cuda:0')\n",
    "    \n",
    "img = imread(f'./images/{image_name}').to(device=device) * 2 - 1 \n",
    "\n",
    "\n",
    "initial_size = (64, 64)\n",
    "final_size = (186, 240)\n",
    "n_iterations = 15\n",
    "scale = ((final_size[0] / initial_size[0]) ** (1 / n_iterations), (final_size[1] / initial_size[1]) ** (1 / n_iterations))\n",
    "\n",
    "for t in [250]:\n",
    "    print(f't={t}')\n",
    "    img = model.sample(image_size=initial_size, batch_size=1)\n",
    "    show_sample(img)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        print(i)\n",
    "        resized_img = resize(img, scale_factors=scale)\n",
    "        show_sample(resized_img)\n",
    "        print(img.shape)\n",
    "        noisy_img = noise_img(resized_img, model, t)\n",
    "        show_sample(noisy_img)\n",
    "        img = model.sample(custom_initial_img=noisy_img, custom_timesteps=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiment - Collage as VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_name = 'man.jpg-woman.jpg'\n",
    "version_name = '2-trained-on-64crops-for-visual-summary-collage'\n",
    "path = f'/home/yanivni/data/remote_projects/single-image-diffusion/lightning_logs/{image_name}/{version_name}/checkpoints/single-level-step=299999.ckpt'\n",
    "model = Diffusion.load_from_checkpoint(path, model=NextNet(depth=16, filters_per_layer=64), timesteps=500, strict=False).to(device='cuda:0')\n",
    "    \n",
    "def summarize_image(img, model, n_iteration=10, scale=(0.9, 0.9), t=100):\n",
    "    resized_imgs = [img]\n",
    "    for i in range(n_iterations):\n",
    "        resized_img = resize(resized_imgs[-1], scale_factors=scale)\n",
    "        fixed_img = model.sample(custom_initial_img = noise_img(resized_img, model, t), custom_timesteps=t)\n",
    "        resized_imgs.append(fixed_img)\n",
    "    return resized_imgs\n",
    "\n",
    "img = imread(f'./images/collage/man_woman_combined.png').to(device=device) * 2 - 1 \n",
    "final_size = (img.shape[-2], img.shape[-1] // 2)\n",
    "\n",
    "n_iterations = 15\n",
    "scale = ((final_size[0] / img.shape[-2]) ** (1 / n_iterations), (final_size[1] / img.shape[-1]) ** (1 / n_iterations))\n",
    "print(scale)\n",
    "\n",
    "for t in [150, 250]:\n",
    "    print(f't={t}')\n",
    "    samples = summarize_image(img, model, n_iterations, scale, t)\n",
    "    sample_directory = os.path.join(saved_data_path, 'Visual Summary', f'{image_name}/t={t}_scale={scale[0] :2f}_{scale[1] :2f}')\n",
    "    os.makedirs(sample_directory, exist_ok=True)\n",
    "    \n",
    "    for i, s in enumerate(samples):\n",
    "        print(f'scale={(scale[0] ** i, scale[1] ** i)}')\n",
    "        show_sample(s, 10)\n",
    "        save_diffusion_sample(s, os.path.join(sample_directory, f'scale={scale[0] ** i :2f}_{scale[1] ** i :2f}.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f961cc1adb1d23b163d13487a7bbea047622fac31372ac86f89c77664d2955"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}