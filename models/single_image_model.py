import torch
import torch.nn as nn

from common_utils.fold2d import fold2d, unfold2d
from models.base_diffusion_model import BaseDiffusionModel


class SingleImageDiffusionModel(BaseDiffusionModel):
    """
    A diffusion model meant to be trained on a single image as input.
    The model is a simple 8-layer convolutional model with GeLU activations, and is trained
    on the image and its augmentations as-is.
    """

    def __init__(self, in_size, t_range, img_channels=3):
        super().__init__(in_size, t_range)

        kernel_size = 3
        self.conv_channels = 64

        self.conv0 = nn.Conv2d(img_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv1 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv2 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv3 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv4 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv5 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv6 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size, padding=kernel_size // 2)
        self.conv7 = nn.Conv2d(self.conv_channels, img_channels, kernel_size, padding=kernel_size // 2)
        self.gelu = nn.GELU()

    def forward(self, x, t):
        _, C, H, W = x.shape
        y = self.conv0(x)
        y = self.gelu(y)
        y = self.conv1(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv2(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv3(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv4(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv5(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv6(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv7(y)
        return y

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)
        return optimizer


class UnfoldedSingleImageDiffusionModel(BaseDiffusionModel):
    """
    A diffusion model meant to be trained on a single image as input.
    The model is a simple 8-layer convolutional model with GeLU activations, where each convolutional layer
    is a 1x1 convolution that is trained on all patches, which are generated by an spatially-preserving unfold
    operation.
    """

    def __init__(self, in_size, t_range, patch_size=3, img_channels=3):
        super().__init__(in_size, t_range)

        self.patch_size = patch_size
        self.unfolded_channels = img_channels * patch_size * patch_size

        self.conv_channels = 64
        self.conv0 = nn.Conv2d(self.unfolded_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv1 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv2 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv3 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv4 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv5 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv6 = nn.Conv2d(self.conv_channels, self.conv_channels, kernel_size=(1, 1))
        self.conv7 = nn.Conv2d(self.conv_channels, self.unfolded_channels, kernel_size=(1, 1))
        self.gelu = nn.GELU()

    def forward(self, x, t):
        B, C, H, W = x.shape

        x_unfolded = unfold2d(x, self.patch_size, stride=1, use_padding=True)
        x_unfolded = x_unfolded.view(B, self.unfolded_channels, H, W)

        y = self.conv0(x_unfolded)
        y = self.gelu(y)
        y = self.conv1(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv2(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv3(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv4(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv5(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv6(y) + self.pos_encoding(t, self.self.conv_channels, H)
        y = self.gelu(y)
        y = self.conv7(y)

        # TODO MAYBE THINK OF A SMARTER WAY TO PERFORM THIS FOLDING (INSTEAD OF JUST SUMMING UP OVERLAPPING PATCHES)
        y = y.view(B, C, self.patch_size, self.patch_size, H, W)
        y_folded = fold2d(y)

        return y_folded

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)
        return optimizer
